# awq_quantization.yaml

# Model configuration
MODEL: "unsloth/Llama-3.2-1B-Instruct"

# Data configuration
DATASET_NAME: "wikitext"
DATASET_SUBNAME: "wikitext-2-raw-v1"
TEXT_COLUMN: "text"                     
SPLIT: "train"

DATA_PATH:
FORMAT_STRING:

# Quantization configuration
llm:
  AutoAWQ:
    ZERO_POINT: True                    # zero point quantization
    W_BIT: 4                            # weight bitwidth
    Q_GROUP_SIZE: 128                   # group size for quantization [default: 128, 64, 32]
    VERSION: "GEMV"                     # quantization version (GEMM or GEMV)

# Job configuration
CUDA_ID: "0"
ALGORITHM: "AutoAWQ"
JOB_SERVICE: "Kompress"
USER_FOLDER: "awq_job_test"
JOB_ID: "awq_quantization"
CACHE_PATH: "awq_job_test/.cache" 
JOB_PATH: "awq_job_test/jobs/awq_quantization"
LOGGING_PATH: "./logger_test/log.log"
ALGO_TYPE: "llm"
TASK: "llm"