# flap_pruning.yaml

# Model configuration
MODEL: "TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T"

# Data configuration
DATASET_NAME: "wikitext"
DATASET_SUBNAME: "wikitext-2-raw-v1"
TEXT_COLUMN: "text"                     
SPLIT: "train"

DATA_PATH:
FORMAT_STRING:

# Quantization configuration
llm:
  FlapPruner:
    dtype: "float16"
    metrics: "WIFV"
    nsamples: 1024
    pruning_ratio: 0.5
    remove_heads: -1
    seed: 0
    start_pruning_layer_idx: 56
    structure: "AL-AM"

    to_finetune: False

# Job configuration
CUDA_ID: "0"
ALGORITHM: "FlapPruner"
JOB_SERVICE: "Kompress"
USER_FOLDER: "user_data"
JOB_ID: "flap_pruning"
CACHE_PATH: "user_data/.cache"
JOB_PATH: "user_data/jobs/flap_pruning"
LOGGING_PATH: "user_data/logs/flap_pruning"
ALGO_TYPE: "llm"
TASK: "llm"