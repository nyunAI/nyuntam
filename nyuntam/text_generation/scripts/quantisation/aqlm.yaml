# test yaml 1: the paths are what is within the docker - /custom_data/<path>

ALGORITHM: AQLM
ALGO_TYPE: "llm"
MODEL: "meta-llama/Meta-Llama-3-8B"
CUSTOM_MODEL_PATH: ""
MODEL_PATH: "user_data/jobs/46"
CACHE_PATH: "user_data/.cache"
JOB_ID: 46
JOB_PATH: "user_data/jobs/46"
JOB_SERVICE: "Kompress"
TASK: "llm"
OUTPUT_PATH: "user_data/models"
USER_FOLDER: "user_data"
LOGGING_PATH: "user_data/logs/46"

DATA_URL: ""
DATASET_NAME: "togethercomputer/RedPajama-Data-1T-Sample"
DATASET_SUBNAME: ""
DATA_PATH: ""
TEXT_COLUMN: "text" # if multiple, separate by comma e.g. 'instruction,input,output'
SPLIT: "train"
FORMAT_STRING: # format string for multicolumned datasets

CUDA_ID: "0"

llm:
  AQLM:
    # Quantization parameters
    save_intermediate_results: true
    dtype: "float16"

    calibration_config:
      attn_implementation: null
      beam_size: 1
      codebook_value_nbits: 16
      codebook_value_num_groups: 1
      dtype: "float16"
      finetune_adam_beta1: 0.9
      finetune_adam_beta2: 0.999
      finetune_batch_size: 16
      finetune_early_stop: 3
      finetune_keep_best: true
      finetune_lr: 0.0001
      finetune_max_epochs: 25
      in_group_size: 8
      init_max_iter: 100
      init_max_points_per_centroid: null
      local_batch_size: 1
      lr: 0.0001
      max_epochs: 100
      mix_compression: false
      model_seqlen: 4096
      nbits_per_codebook: 16
      new_eval: false
      no_quant: false
      nsamples: 2048
      num_codebooks: 1
      offload_activations: true
      on_save: null
      out_group_size: 1
      print_frequency: 10
      relative_mse_tolerance: 0.01
      resume: false
      scale_nbits: 0
      seed: 0
      skip_out_loss: false
      steps_per_epoch: 100
      true_sequential: false
      trust_remote_code: true
      use_checkpointing: false
      use_faiss: false
      use_fast_tokenizer: false
      val_size: 256
      wandb: false
    conversion_config:
      attn_implementation: null
      code_dtype: int32
      load_dtype: auto
      trust_remote_code: true
    finetune_config:
      adam_beta1: 0.9
      adam_beta2: 0.95
      amp_dtype: float32
      amsgrad: false
      attn_implementation: null
      base_model: base_model
      batch_size: 1 # 1 gpu
      beam_size: 1
      block_type: LlamaDecoderLayer
      code_adam_16bit: false
      code_beta1: 0.0
      code_beta2: 0.95
      code_dtype: uint16
      code_lr: 0.001
      code_selection_temperature: 0
      code_trust_ratio: 0.01
      debias: true
      delta_decay: 0
      download_num_workers: null
      eval_datasets:
      - wikitext2
      - c4
      eval_every_steps: 1
      force_code_update: false
      gradient_checkpointing: true
      keep_best_model: false
      lamb: true
      limit_parallel_inits: 1 # 1 gpu
      load_dtype: float32
      lr: 0.0001
      master_dtype: float32
      max_code_change_per_step: 0.01
      max_epochs: 10
      microbatch_size: 1 # 1 gpu
      minimize_sync: false
      model_seqlen: 4096
      monkeypatch_old_pickle: false
      num_workers: 8
      overwrite_cache: false
      preprocessing_chunk_length: null
      preprocessing_keep_in_memory: false
      preprocessing_num_workers: 24
      print_every_steps: 1
      save_every_steps: 1
      seed: 1337
      skip_grouping: true
      straight_through_buffer_dtype: float32
      trust_remote_code: true
      update_codebooks_and_scales: true
      update_codes: true
      update_non_quantized_parameters: true
      use_fast_tokenizer: false
      use_fsdp_amp: false
      verbose_optimizer: true
      wandb: false
      wrap_separately: []
