#Training Arguments
ADAM_EPS: 1e-8
BATCH_SIZE: 4
BETA1: 0.9
BETA2: 0.999
CACHE_BOOL: false
DO_EVAL: false
DO_TRAIN: true
EPOCHS: 0.01
GRADIENT_ACCUMULATION_STEPS: 1
GRADIENT_CHECKPOINTING: true
GROUP_BY_LENGTH: true
INTERVAL: steps
INTERVAL_STEPS: 50
LR: 0.0002
NO_OF_CHECKPOINTS: 5
NUM_WORKERS: 4
OPTIMIZER: paged_adamw_32bit
REMOVE_UNUSED_COLUMNS: true
RESUME_FROM_CHECKPOINT: false

#Dataset Arguments
CUSTOM_DATASET_PATH: null
DATASET: mlabonne/guanaco-llama2-1k
DATASET_CONFIG: {}
DATASET_FORMAT: null
DATASET_ID: 27
FP16: true
SCHEDULER_TYPE: constant
SEED: 56
STEPS: 100
WEIGHT_DECAY: 0.001
alpha: 16
blocksize: 128
dataset_text_field: text
max_seq_length: 512
packing: true

#Model Arguments
LOCAL_MODEL_PATH: null
MODEL: Llama-2
MODEL_PATH: NousResearch/Llama-2-7b-hf

#Basic Arguments
ID: 27
JOB_ID: 100
JOB_SERVICE: Adapt
LOGGING_PATH: /user_data/logs/Adapt/100
OUTPUT_DIR: /user_data/jobs/Adapt/100
OVERWRITE_OUTPUT_DIR: false
Library: Huggingface
FSDP: true
SAVE_METHOD: state_dict
TASK: text_generation
USER_FOLDER: user_data
cuda_id: '0,1,2,3'
num_nodes: 1

#PEFT arguments
FULL_FINE_TUNING: false
LAST_LAYER_TUNING: true
PEFT_METHOD: DoRA
dropout: 0.1
fan_in_fan_out: false
flash_attention2: false
init_lora_weights: true
peft_type: DoRA
r: 2
target_modules: null
auto_select_modules: true

#Quantization Arguments
load_in_4bit: true
bnb_4bit_compute_dtype: float16
bnb_4bit_quant_type: nf4
bnb_4bit_use_double_quant: false

#FSDP configs
compute_environment: LOCAL_MACHINE
debug: true
distributed_type: FSDP
downcast_bf16: 'no'
enable_cpu_affinity: false
fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP
fsdp_backward_prefetch: NO_PREFETCH
fsdp_cpu_ram_efficient_loading: true
fsdp_forward_prefetch: false
fsdp_offload_params: false
fsdp_sharding_strategy: FULL_SHARD
fsdp_state_dict_type: SHARDED_STATE_DICT
fsdp_sync_module_states: true
fsdp_transformer_layer_cls_to_wrap: null
fsdp_use_orig_params: true
machine_rank: 0
main_training_function: main
mixed_precision: 'no'
num_machines: 1
num_processes: 4
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: true