TASK : Seq2Seq_tasks
subtask : summarization
max_input_length : 512
max_target_length : 128
eval_metric : 'rouge' 
cuda_id : '0'
OUTPUT_DIR : './results_txt'
OVERWRITE_OUTPUT_DIR : False
LOGGING_PATH: 'test_log.log'
packing : True
dataset_text_field : 'text' 
max_seq_length : 512
flash_attention2 : false
blocksize : 128
SAVE_METHOD : 'state_dict'


# DATASET_ARGS :
DATASET : 'xsum'
DATA_VERSION : '1.0'
DATA_SPLIT : 
TRAIN_DIR : ''
VAL_DIR : ''
TEST_DIR : ''
MAX_TRAIN_SAMPLES : 1000
MAX_EVAL_SAMPLES : 1000
CUSTOM_DATASET_PATH : #'abc/dataset' 
DATASET_CONFIG : {}
input_column : 'document'
target_column : 'summary'

# MODEL_ARGS :
MODEL : "t5"
MODEL_PATH :  'google-t5/t5-large'
MODEL_VERSION : '1.0'
CACHE_BOOL : False

# TRAINING_ARGS :
SEED : 56
DO_TRAIN : True
DO_EVAL : True
NUM_WORKERS : 4
BATCH_SIZE : 16
EPOCHS : 1
STEPS : 1
OPTIMIZER : 'adamw_torch'
LR : 1e-4
SCHEDULER_TYPE : 'linear'
WEIGHT_DECAY : 0.0
BETA1 : 0.9
BETA2 : 0.999
ADAM_EPS : 1e-8 
INTERVAL : 'epoch'
INTERVAL_STEPS : 100
NO_OF_CHECKPOINTS : 5
FP16 : False
RESUME_FROM_CHECKPOINT : False
GRADIENT_ACCUMULATION_STEPS : 1
GRADIENT_CHECKPOINTING : True
predict_with_generate: True
generation_max_length : 128
REMOVE_UNUSED_COLUMNS : True

# FINE_TUNING_ARGS :
LAST_LAYER_TUNING : True
FULL_FINE_TUNING : False

PEFT_METHOD : 'LoRA'

# LoRA_CONFIG :
r : 16
alpha : 8
dropout : 0.1
peft_type : 'LoRA'
target_modules : 
fan_in_fan_out : False
init_lora_weights : True  

# BNB_CONFIG :
load_in_4bit : True
bnb_4bit_compute_dtype : "float16"
bnb_4bit_quant_type : "nf4"
bnb_4bit_use_double_quant : False 