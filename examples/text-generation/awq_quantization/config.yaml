# awq_quantization.yaml

# Model configuration
MODEL: "meta-llama/Meta-Llama-3.1-8B-Instruct"

# Data configuration
DATASET_NAME: "wikitext"
DATASET_SUBNAME: "wikitext-2-raw-v1"
TEXT_COLUMN: "text"                     
SPLIT: "train"

DATA_PATH:
FORMAT_STRING:

# Quantization configuration
llm:
  AutoAWQ:
    ZERO_POINT: True                    # zero point quantization
    W_BIT: 4                            # weight bitwidth
    Q_GROUP_SIZE: 128                   # group size for quantization [default: 128, 64, 32]
    VERSION: "GEMV"                     # quantization version (GEMM or GEMV)

# Job configuration
CUDA_ID: "0"
ALGORITHM: "AutoAWQ"
JOB_SERVICE: "Kompress"
USER_FOLDER: "user_data"
JOB_ID: "awq_quantization"
CACHE_PATH: "user_data/.cache"
JOB_PATH: "user_data/jobs/awq_quantization"
LOGGING_PATH: "user_data/logs/awq_quantization"
ALGO_TYPE: "llm"
TASK: "llm"